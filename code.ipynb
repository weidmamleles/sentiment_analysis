{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabalho de Grupo - Análise de Sentimento\n",
    "Para este trabalho decidimos utilizar o dataset SFU Review Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import nltk.stem as stem\n",
    "from nltk import FeatDict\n",
    "from nltk.classify import naivebayes, maxent\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import collections\n",
    "import string\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"TM/data/en-sentiment/SFU_Review_Corpus_train.csv\", encoding=\"utf-8\")\n",
    "df_test = pd.read_csv(\"TM/data/en-sentiment/SFU_Review_Corpus_test.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = df_test['text'].tolist()\n",
    "test_recommended = df_test['recommended'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = df_train['text'].tolist()\n",
    "train_recommended = df_train['recommended'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 - Preparação dos dados e criação de uma baseline\n",
    "\n",
    "### 1.2.1 - TexBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test = list(zip(df_test['text'], df_test['recommended']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "somaclassificacao =0\n",
    "for text, recommended in list_test:\n",
    "    classification = TextBlob(text).sentiment.polarity\n",
    "    if(classification <0 and recommended == \"no\"):\n",
    "        somaclassificacao +=1\n",
    "    elif(classification >=0 and recommended ==\"yes\"):\n",
    "        somaclassificacao +=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Accuracy do TextBlob é 0.6375\n"
     ]
    }
   ],
   "source": [
    "print (\"A Accuracy do TextBlob é\", somaclassificacao/len(list_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Aplicação de um léxico de sentimentos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Sem tratamento da negação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leitura do léxico\n",
    "lex={}\n",
    "with open(\"TM/data/en/NCR-lexicon.csv\", encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter=\",\" )\n",
    "    for i, d in enumerate(reader):\n",
    "        lex[ d[\"English\"] ] = int(d[\"Positive\"]) - int(d[\"Negative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leitura dos dados de teste, a variável temp serve para não perdermos a lista original\n",
    "list_test_data=[]\n",
    "temp=[]\n",
    "with open(\"TM/data/en-sentiment/SFU_Review_Corpus_test.csv\", encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile )\n",
    "    for d in reader:\n",
    "        list_test_data.append( (d[\"text\"],d[\"recommended\"]) )\n",
    "        temp.append( (d[\"text\"],d[\"recommended\"]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divisão dos dados por palavras através do .split()\n",
    "for i, text in enumerate(list_test_data):\n",
    "    list_test_data[i] = text[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contagem dos scores de cada review\n",
    "count_pos=0\n",
    "count_neg=0\n",
    "\n",
    "list_scores=[]\n",
    "\n",
    "for i in range(len(list_test_data)):\n",
    "    for j in range(len(list_test_data[i])):\n",
    "        if list_test_data[i][j] in lex:\n",
    "            valor = lex[str(list_test_data[i][j])]\n",
    "            if valor == 1 :\n",
    "                count_pos+=1\n",
    "            elif valor == -1 :\n",
    "                count_neg+=1\n",
    "        \n",
    "    list_scores.append((count_pos - count_neg)/(count_pos + count_neg + 2))\n",
    "    count_pos=0\n",
    "    count_neg=0\n",
    "        \n",
    "#print(list_scores)\n",
    "#print(temp[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criação da lista final com a review e com a classificação de acordo com os scores obtidos\n",
    "list_final = []\n",
    "\n",
    "for i in range(len(list_scores)):\n",
    "    if list_scores[i] >=0 :\n",
    "        list_final.append([temp[i][0],\"yes\"])\n",
    "    else :\n",
    "        list_final.append([temp[i][0],\"no\"])\n",
    "        \n",
    "#list_final[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criação da lista apenas com as novas classificações para ser utilizada no cálculo da accuracy\n",
    "test_recommended_pred = []\n",
    "\n",
    "for i in range(len(list_final)):\n",
    "    test_recommended_pred.append(list_final[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Accuracy do Lexicon é:   0.562\n"
     ]
    }
   ],
   "source": [
    "#Cálculo da accuracy\n",
    "score = sklearn.metrics.accuracy_score(test_recommended,test_recommended_pred)\n",
    "print(\"A Accuracy do Lexicon é:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Com tratamento da negação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leitura do léxico\n",
    "lex2={}\n",
    "with open(\"TM/data/en/NCR-lexicon.csv\", encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter=\",\" )\n",
    "    for i, d in enumerate(reader):\n",
    "        lex2[ d[\"English\"] ] = int(d[\"Positive\"]) - int(d[\"Negative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leitura dos dados de teste, a variável temp2 e temp3 servem para não perdermos a lista original\n",
    "#Neste caso sentimos a necessidade de criar duas variáveis temp, uma vez que tivémos que utilizar uma para aplicar a negação\n",
    "\n",
    "list_test_data2=[]\n",
    "temp2=[]\n",
    "temp3=[]\n",
    "\n",
    "with open(\"TM/data/en-sentiment/SFU_Review_Corpus_test.csv\", encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile )\n",
    "    for d in reader:\n",
    "        list_test_data2.append( (d[\"text\"],d[\"recommended\"]) )\n",
    "        temp2.append( (d[\"text\"],d[\"recommended\"]) )\n",
    "        temp3.append( (d[\"text\"],d[\"recommended\"]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divisão dos dados por palavras através do .split()\n",
    "for i, text in enumerate(list_test_data2):\n",
    "    list_test_data2[i] = text[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, text in enumerate(temp2):\n",
    "    temp2[i] = text[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicação do tratamento da negação\n",
    "list_word_neg = ['not', 'no', \"can't\", \"haven't\", \"isn't\", \"aren't\", \"won't\", 'nor', \"doesn't\",\"don't\",\n",
    "                 \"couldn't\", \"daren't\", \"didn't\", \"hasn't\", \"hadn't\", \"mayn't\", \"mightn't\", \"mustn't\",\n",
    "                 \"needn't\", \"oughtn't\", \"shan't\", \"shouldn't\", \"wasn't\", \"weren't\", \"won't\", \"wouldn't\", \"mayn't\", \n",
    "                 \"Ain't\"\n",
    "                ]\n",
    "delims = [\"?\",\".\",\",\",\"!\",\":\",\";\"]\n",
    "\n",
    "for i in range(len(list_test_data2)):\n",
    "    for j in range(len(list_test_data2[i])):\n",
    "        tmp = list_test_data2[i][j]\n",
    "        if list_test_data2[i][j] in list_word_neg:\n",
    "            list_test_data2[i][j+1] = (\"NOT_\" + list_test_data2[i][j+1])\n",
    "        elif \"NOT_\" in list_test_data2[i][j-1] and j!= 0:\n",
    "            list_test_data2[i][j] = (\"NOT_\" + list_test_data2[i][j])\n",
    "            for p in delims:\n",
    "                if p in list_test_data2[i][j-1]:\n",
    "                    list_test_data2[i][j] = tmp\n",
    "                    \n",
    "#print(list_test_data2[62])\n",
    "#print(list_word_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contagem dos scores de cada review\n",
    "count_pos=0\n",
    "count_neg=0\n",
    "\n",
    "list_scores2=[]\n",
    "\n",
    "for i in range(len(temp2)):\n",
    "    for j in range(len(temp2[i])):\n",
    "        if temp2[i][j] in lex2 and \"NOT_\" in list_test_data2[i][j]:\n",
    "            valor = lex2[str(temp2[i][j])] * (-1)\n",
    "        elif temp2[i][j] in lex2 and \"NOT_\" not in list_test_data2[i][j]: \n",
    "            valor = lex2[str(temp2[i][j])]\n",
    "        if valor == 1 :\n",
    "            count_pos+=1\n",
    "        elif valor == -1 :\n",
    "            count_neg+=1\n",
    "        \n",
    "    list_scores2.append((count_pos - count_neg)/(count_pos + count_neg + 2))\n",
    "    count_pos=0\n",
    "    count_neg=0\n",
    "\n",
    "#print(len(list_scores2))\n",
    "#print(list_scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criação da lista final com a review e com a classificação de acordo com os scores obtidos\n",
    "list_final2 = []\n",
    "\n",
    "for i in range(len(list_scores2)):\n",
    "    if list_scores2[i] >=0 :\n",
    "        list_final2.append([temp3[i][0],\"yes\"])\n",
    "    else :\n",
    "        list_final2.append([temp3[i][0],\"no\"])\n",
    "        \n",
    "#list_final[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criação da lista apenas com as novas classificações para ser utilizada no cálculo da accuracy\n",
    "test_recommended_pred2 = []\n",
    "\n",
    "for i in range(len(list_final2)):\n",
    "    test_recommended_pred2.append(list_final2[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Accuracy do Lexicon com tratamento da negação é:   0.600\n"
     ]
    }
   ],
   "source": [
    "#Cálculo da accuracy\n",
    "score = sklearn.metrics.accuracy_score(test_recommended,test_recommended_pred2)\n",
    "print(\"A Accuracy do Lexicon com tratamento da negação é:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Pre-processamento e sem tratamento da negação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leitura do léxico\n",
    "lex3={}\n",
    "with open(\"TM/data/en/NCR-lexicon.csv\", encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter=\",\" )\n",
    "    for i, d in enumerate(reader):\n",
    "        lex3[ d[\"English\"] ] = int(d[\"Positive\"]) - int(d[\"Negative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leitura dos dados de teste, a variável temp4 serve para não perdermos a lista original\n",
    "list_test_data3=[]\n",
    "temp4=[]\n",
    "\n",
    "with open(\"TM/data/en-sentiment/SFU_Review_Corpus_test.csv\", encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile )\n",
    "    for d in reader:\n",
    "        list_test_data3.append( (d[\"text\"],d[\"recommended\"]) )\n",
    "        temp4.append( (d[\"text\"],d[\"recommended\"]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divisão dos dados por palavras através do .split()\n",
    "for i, text in enumerate(list_test_data3):\n",
    "    list_test_data3[i] = text[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Faz Lemmatização\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(list_test_data3)):\n",
    "    for j in range(len(list_test_data3[i])):\n",
    "        list_test_data3[i][j] = lemmatizer.lemmatize(list_test_data3[i][j], 'v') #verbo\n",
    "        list_test_data3[i][j] = lemmatizer.lemmatize(list_test_data3[i][j], 'a') #adjetivo\n",
    "        list_test_data3[i][j] = lemmatizer.lemmatize(list_test_data3[i][j], 'n') #nomes\n",
    "        list_test_data3[i][j] = lemmatizer.lemmatize(list_test_data3[i][j], 's') #adjetivo saturado\n",
    "        list_test_data3[i][j] = lemmatizer.lemmatize(list_test_data3[i][j], 'r') #adverbio\n",
    "\n",
    "\n",
    "#print(list_test_data4[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicação do stemming\n",
    "stemmer = stem.porter.PorterStemmer()\n",
    "\n",
    "for i in range(len(list_test_data3)):\n",
    "    for j in range(len(list_test_data3[i])):\n",
    "        list_test_data3[i][j] = stemmer.stem(list_test_data3[i][j])\n",
    "\n",
    "#print(list_test_data3[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contagem dos scores de cada review\n",
    "count_pos=0\n",
    "count_neg=0\n",
    "\n",
    "list_scores3=[]\n",
    "\n",
    "for i in range(len(list_test_data3)):\n",
    "    for j in range(len(list_test_data3[i])):\n",
    "        if list_test_data3[i][j] in lex3:\n",
    "            valor = lex3[str(list_test_data3[i][j])]\n",
    "            if valor == 1 :\n",
    "                count_pos+=1\n",
    "            elif valor == -1 :\n",
    "                count_neg+=1\n",
    "        \n",
    "    list_scores3.append((count_pos - count_neg)/(count_pos + count_neg + 2))\n",
    "    count_pos=0\n",
    "    count_neg=0\n",
    "        \n",
    "#print(list_scores)\n",
    "#print(temp[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criação da lista final com a review e com a classificação de acordo com os scores obtidos\n",
    "list_final3 = []\n",
    "\n",
    "for i in range(len(list_scores3)):\n",
    "    if list_scores3[i] >=0 :\n",
    "        list_final3.append([temp4[i][0],\"yes\"])\n",
    "    else :\n",
    "        list_final3.append([temp4[i][0],\"no\"])\n",
    "        \n",
    "#list_final3[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criação da lista apenas com as novas classificações para ser utilizada no cálculo da accuracy\n",
    "test_recommended_pred3 = []\n",
    "\n",
    "for i in range(len(list_final3)):\n",
    "    test_recommended_pred3.append(list_final3[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Accuracy do Lexicon com stemming:   0.625\n"
     ]
    }
   ],
   "source": [
    "#Cálculo da accuracy\n",
    "score = sklearn.metrics.accuracy_score(test_recommended,test_recommended_pred3)\n",
    "print(\"A Accuracy do Lexicon com stemming:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 Pre-processamento e com tratamento da negação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leitura do léxico\n",
    "lex4={}\n",
    "with open(\"TM/data/en/NCR-lexicon.csv\", encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter=\",\" )\n",
    "    for i, d in enumerate(reader):\n",
    "        lex4[ d[\"English\"] ] = int(d[\"Positive\"]) - int(d[\"Negative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leitura dos dados de teste, a variável temp5 e temp6 servem para não perdermos a lista original\n",
    "#Neste caso sentimos a necessidade de criar duas variáveis temp, uma vez que tivémos que utilizar uma para aplicar a negação\n",
    "list_test_data4=[]\n",
    "temp5=[]\n",
    "temp6=[]\n",
    "\n",
    "with open(\"TM/data/en-sentiment/SFU_Review_Corpus_test.csv\", encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile )\n",
    "    for d in reader:\n",
    "        list_test_data4.append( (d[\"text\"],d[\"recommended\"]) )\n",
    "        temp5.append( (d[\"text\"],d[\"recommended\"]) )\n",
    "        temp6.append( (d[\"text\"],d[\"recommended\"]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, text in enumerate(temp5):\n",
    "    temp5[i] = text[0].split()\n",
    "#print(temp5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divisão dos dados por palavras através do .split()\n",
    "for i, text in enumerate(list_test_data4):\n",
    "    list_test_data4[i] = text[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicação do tratamento da negação\n",
    "list_word_neg = ['not', 'no', \"can't\", \"haven't\", \"isn't\", \"aren't\", \"won't\", 'nor', \"doesn't\",\"don't\",\n",
    "                 \"couldn't\", \"daren't\", \"didn't\", \"hasn't\", \"hadn't\", \"mayn't\", \"mightn't\", \"mustn't\",\n",
    "                 \"needn't\", \"oughtn't\", \"shan't\", \"shouldn't\", \"wasn't\", \"weren't\", \"won't\", \"wouldn't\", \"mayn't\", \n",
    "                 \"ain't\", \"NOT\", \"Not\"\n",
    "                ]\n",
    "\n",
    "delims = [\"?\",\".\",\",\",\"!\",\":\",\";\"]\n",
    "\n",
    "for i in range(len(list_test_data4)):\n",
    "    for j in range(len(list_test_data4[i])):\n",
    "        tmp = list_test_data4[i][j]\n",
    "        if list_test_data4[i][j] in list_word_neg:\n",
    "            list_test_data4[i][j+1] = (\"NOT_\" + list_test_data4[i][j+1])\n",
    "        elif \"NOT_\" in list_test_data4[i][j-1] and j!= 0:\n",
    "            list_test_data4[i][j] = (\"NOT_\" + list_test_data4[i][j])\n",
    "            for p in delims:\n",
    "                if p in list_test_data4[i][j-1]:\n",
    "                    list_test_data4[i][j] = tmp\n",
    "                    \n",
    "#print(list_test_data4[:2])\n",
    "#print(list_word_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Faz Lemmatização\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(list_test_data4)):\n",
    "    for j in range(len(list_test_data4[i])):\n",
    "        list_test_data4[i][j] = lemmatizer.lemmatize(list_test_data4[i][j], 'v') #verbo\n",
    "        list_test_data4[i][j] = lemmatizer.lemmatize(list_test_data4[i][j], 'a') #adjetivo\n",
    "        list_test_data4[i][j] = lemmatizer.lemmatize(list_test_data4[i][j], 'n') #nomes\n",
    "        list_test_data4[i][j] = lemmatizer.lemmatize(list_test_data4[i][j], 's') #adjetivo saturado\n",
    "        list_test_data4[i][j] = lemmatizer.lemmatize(list_test_data4[i][j], 'r') #adverbio\n",
    "\n",
    "\n",
    "#print(list_test_data4[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicação do stemming\n",
    "stemmer = stem.porter.PorterStemmer()\n",
    "\n",
    "for i in range(len(list_test_data4)):\n",
    "    for j in range(len(list_test_data4[i])):\n",
    "        list_test_data4[i][j] = stemmer.stem(list_test_data4[i][j])\n",
    "\n",
    "#print(list_test_data4[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contagem dos scores de cada review\n",
    "count_pos=0\n",
    "count_neg=0\n",
    "\n",
    "list_scores4=[]\n",
    "\n",
    "for i in range(len(temp5)):\n",
    "    for j in range(len(temp5[i])):\n",
    "        if temp5[i][j] in lex4 and \"not_\" in list_test_data4[i][j]:\n",
    "            valor = lex4[str(temp5[i][j])] * (-1)\n",
    "        elif temp5[i][j] in lex4 and \"not_\" not in list_test_data4[i][j]: \n",
    "            valor = lex4[str(temp5[i][j])]\n",
    "        if valor == 1 :\n",
    "            count_pos+=1\n",
    "        elif valor == -1 :\n",
    "            count_neg+=1\n",
    "        \n",
    "    list_scores4.append((count_pos - count_neg)/(count_pos + count_neg + 2))\n",
    "    count_pos=0\n",
    "    count_neg=0\n",
    "\n",
    "#print(len(list_scores2))\n",
    "#print(list_scores4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_final4 = []\n",
    "\n",
    "for i in range(len(list_scores4)):\n",
    "    if list_scores4[i] >=0 :\n",
    "        list_final4.append([temp6[i][0],\"yes\"])\n",
    "    else :\n",
    "        list_final4.append([temp6[i][0],\"no\"])\n",
    "        \n",
    "#list_final4[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recommended_pred4 = []\n",
    "\n",
    "for i in range(len(list_final4)):\n",
    "    test_recommended_pred4.append(list_final4[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Accuracy do Lexicon com tratamento da negação e stemming é:   0.613\n"
     ]
    }
   ],
   "source": [
    "score = sklearn.metrics.accuracy_score(test_recommended,test_recommended_pred4)\n",
    "print(\"A Accuracy do Lexicon com tratamento da negação e stemming é:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 - Aprendizagem automática\n",
    "## 2.1 - Aprendizagem automática a usar a biblioteca NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LÊ CSV COM OS DADOS DO CONJUNTO DE TREINO\n",
    "recommended=('yes', 'no')\n",
    "text = []\n",
    "y = []\n",
    "\n",
    "with open(\"TM/data/en-sentiment/SFU_Review_Corpus_train.csv\", encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for i, row in enumerate(reader):\n",
    "        if row[\"recommended\"] == recommended[1]:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "        text.append(row[\"text\"])\n",
    "        \n",
    "        \n",
    "#LÊ CSV COM OS DADOS DO CONJUNTO DE TESTE        \n",
    "recommended_testing=('yes', 'no')\n",
    "Test_text = []\n",
    "test_y = []\n",
    "\n",
    "with open(\"TM/data/en-sentiment/SFU_Review_Corpus_test.csv\", encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for i, row in enumerate(reader):\n",
    "        if row[\"recommended\"] == recommended_testing[1]:\n",
    "            test_y.append(1)\n",
    "        else:\n",
    "            test_y.append(0)\n",
    "        Test_text.append(row[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treino\n",
    "#tokenizer = word_tokenize()\n",
    "docs = []\n",
    "\n",
    "for words in text:\n",
    "    doc = collections.Counter()\n",
    "    for w in word_tokenize(words):\n",
    "        doc[w] += 1\n",
    "    docs.append(doc)\n",
    "    \n",
    "#test\n",
    "#tokenizer_test = word_tokenize()\n",
    "docs_test = []\n",
    "\n",
    "for words in Test_text:\n",
    "    doc_test = collections.Counter()\n",
    "    for w in word_tokenize(words):\n",
    "        doc_test[w] += 1\n",
    "    docs_test.append(doc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VOCABULÁRIO TREINO\n",
    "voc_length = 1500 #DEFINE A QUANTIDADE DE FEATURES\n",
    "\n",
    "tf = collections.Counter()\n",
    "df = collections.Counter()\n",
    "\n",
    "for d in docs:\n",
    "    for w in d:\n",
    "        tf[w] += d[w]\n",
    "        df[w] += 1\n",
    "\n",
    "idfs = {}\n",
    "for w in tf:\n",
    "    if tf[w] > 2:\n",
    "        idfs[w] = np.log(len(docs)/df[w])\n",
    "        \n",
    "#CRIA O VOCÁBULÁRIO COM AS FEATURES MAIS IMPORTANTES DO CONJUNTO DE TREINO\n",
    "voc = sorted(idfs, key=idfs.get, reverse=True)[:voc_length]\n",
    "\n",
    "\n",
    "#VOCABULÁRIO TESTE\n",
    "test_voc_length = 1500 #DEFINE A QUANTIDADE DE FEATURES\n",
    "\n",
    "tf_test = collections.Counter()\n",
    "df_test = collections.Counter()\n",
    "\n",
    "for d in docs_test:\n",
    "    for w in d:\n",
    "        tf_test[w] += d[w]\n",
    "        df_test[w] += 1\n",
    "\n",
    "idfs_test = {}\n",
    "for w in tf_test:\n",
    "    if tf_test[w] > 2:\n",
    "        idfs_test[w] = np.log(len(docs_test)/df_test[w])\n",
    "        \n",
    "#CRIA O VOCÁBULÁRIO COM AS FEATURES MAIS IMPORTANTES DO CONJUNTO DE TESTE\n",
    "voc_test = sorted(idfs_test, key=idfs_test.get, reverse=True)[:test_voc_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cria um índice para o conjunto treino - ou seja, associa o vocabulário a um índice\n",
    "indice = {}\n",
    "for i,w in enumerate(sorted(voc)):\n",
    "    indice[w] = i\n",
    "    \n",
    "#cria um índice para o conjunto teste - ou seja, associa o vocabulário a um índice\n",
    "indice_test = {}\n",
    "for i,w in enumerate(sorted(voc_test)):\n",
    "    indice_test[w] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treino\n",
    "docrep = []\n",
    "for d in docs:\n",
    "    valores = np.zeros([len(voc)])\n",
    "    for w in d:\n",
    "        if w in indice:\n",
    "            valores[ indice[w] ] = d[w]\n",
    "    docrep.append (valores)\n",
    "    \n",
    "#teste    \n",
    "docrep_test = []\n",
    "for d in docs_test:\n",
    "    valores_test = np.zeros([len(voc_test)])\n",
    "    for w in d:\n",
    "        if w in indice_test:\n",
    "            valores_test[indice_test[w] ] = d[w]\n",
    "    docrep_test.append (valores_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treino\n",
    "newdocrep = []\n",
    "for d,c in zip(docs, y):\n",
    "    docwords={}\n",
    "    for w in d:\n",
    "        if w in indice:\n",
    "            docwords[w] = d[w]\n",
    "    newdocrep.append ( (docwords, recommended[c] ) )\n",
    "\n",
    "#teste\n",
    "newdocrep_test = []\n",
    "for d,c in zip(docs_test, test_y):\n",
    "    docwords_test={}\n",
    "    for w in d:\n",
    "        if w in indice_test:\n",
    "            docwords_test[w] = d[w]\n",
    "    newdocrep_test.append ( (docwords_test, recommended_testing[c] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94375"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbc = naivebayes.NaiveBayesClassifier.train(newdocrep)\n",
    "nltk.classify.accuracy(nbc,newdocrep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (15 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.491\n",
      "             2          -0.48233        0.912\n",
      "             3          -0.38165        0.925\n",
      "             4          -0.32115        0.928\n",
      "             5          -0.28037        0.928\n",
      "             6          -0.25084        0.931\n",
      "             7          -0.22838        0.941\n",
      "             8          -0.21067        0.944\n",
      "             9          -0.19631        0.944\n",
      "            10          -0.18441        0.944\n",
      "            11          -0.17436        0.944\n",
      "            12          -0.16577        0.944\n",
      "            13          -0.15831        0.944\n",
      "            14          -0.15178        0.944\n",
      "         Final          -0.14601        0.944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.94375"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mec = maxent.MaxentClassifier.train(newdocrep, bernoulli=False,max_iter=15, trace=3)\n",
    "nltk.classify.accuracy(mec,newdocrep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cria variável final para a classificação do conjunto de teste\n",
    "final_test = [(text) for text, recommended in newdocrep_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FAZ A CLASSIFICAÇÃO DO CONJUNTO DE TESTE ULIZANDO DOIS MÉTODOS: \n",
    "\n",
    "#NBC - Naive Bayes\n",
    "final_recommended_testing = nbc.classify_many(final_test)\n",
    "\n",
    "#MEC - Maximum Entropy (Logistic Regression)\n",
    "final_recommended_testing2 = mec.classify_many(final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRIA A FUNÇÃO PARA CALCULAR A ACCURACY DO NLTK APLICADO A OUTROS CONJUNTOS DE DADOS\n",
    "def accuracy_nltk(reference, test):\n",
    "    if len(reference) != len(test):\n",
    "        raise ValueError(\"Lists must have the same length.\")\n",
    "    return sum(x == y for x, y in zip(reference, test)) / len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "#NBC\n",
    "NBC = accuracy_nltk(test_recommended,final_recommended_testing)\n",
    "print(NBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5375\n"
     ]
    }
   ],
   "source": [
    "#MEC\n",
    "MEC = accuracy_nltk(test_recommended,final_recommended_testing2)\n",
    "print(MEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.56875\n"
     ]
    }
   ],
   "source": [
    "NLTK_accuracy_media = (NBC + MEC) /2\n",
    "print(NLTK_accuracy_media)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Aprendizagem automática a usar a biblioteca SKLEARN - CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset os inputs\n",
    "df_train = pd.read_csv(\"TM/data/en-sentiment/SFU_Review_Corpus_train.csv\", encoding=\"utf-8\")\n",
    "df_test = pd.read_csv(\"TM/data/en-sentiment/SFU_Review_Corpus_test.csv\", encoding=\"utf-8\")\n",
    "test_text = df_test['text'].tolist()\n",
    "test_recommended = df_test['recommended'].tolist()\n",
    "train_text = df_train['text'].tolist()\n",
    "train_recommended = df_train['recommended'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 1500) (80, 1500)\n"
     ]
    }
   ],
   "source": [
    "#converte palavras em números usando a abordagem do bag of words e define que vamos utilizar as 1500 palavras mais ocorrentes como features\n",
    "#Min_df = 3 significa que essas palavras têm de ocorrer em pelo menos 3 comentários\n",
    "#max_df = 0.7 significa que vamos incluir as palavras que ocorrem em no máximo 70% de todos os documentos, uma vez Palavras que ocorrem em quase todos os documentos geralmente não são adequadas para classificação porque não fornecem informações exclusivas sobre o documento.\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=3, max_df=0.7, stop_words= 'english')\n",
    "train_X = vectorizer.fit_transform(train_text)\n",
    "test_X = vectorizer.transform(test_text)\n",
    "print(train_X.shape, test_X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.todense()\n",
    "test_X = test_X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 32 out of 80\n",
      "Accuracy:  0.6\n",
      "Precision:  0.5991244527829893\n",
      "Recall:  0.5996228786926461\n",
      "F1-measure:  0.5989974937343359\n"
     ]
    }
   ],
   "source": [
    "#cria modelo \n",
    "nb = MultinomialNB()\n",
    "model_count_nb = nb.fit(train_X,train_recommended)\n",
    "y_pred = model_count_nb.predict(test_X)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n",
    "\n",
    "nb_count = metrics.accuracy_score(test_recommended, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 22 out of 80\n",
      "Accuracy:  0.725\n",
      "Precision:  0.7458854509545754\n",
      "Recall:  0.7347580138277813\n",
      "F1-measure:  0.7234443746071653\n"
     ]
    }
   ],
   "source": [
    "#cria modelo \n",
    "lr = LogisticRegression(max_iter=500)\n",
    "model_count_lr=lr.fit(train_X, train_recommended)\n",
    "y_pred = model_count_lr.predict(test_X)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n",
    "\n",
    "lr_count = metrics.accuracy_score(test_recommended, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 23 out of 80\n",
      "Accuracy:  0.7125\n",
      "Precision:  0.7366666666666667\n",
      "Recall:  0.7231301068510372\n",
      "F1-measure:  0.7102818453786804\n"
     ]
    }
   ],
   "source": [
    "#cria modelo \n",
    "svmc = LinearSVC(max_iter=500)\n",
    "model_count_svc = svmc.fit(train_X, train_recommended)\n",
    "y_pred = model_count_svc.predict(test_X)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n",
    "\n",
    "svmc_count = metrics.accuracy_score(test_recommended, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 27 out of 80\n",
      "Accuracy:  0.6625\n",
      "Precision:  0.671994884910486\n",
      "Recall:  0.669076052796983\n",
      "F1-measure:  0.6620247222656861\n"
     ]
    }
   ],
   "source": [
    "#cria modelo \n",
    "gnb = GaussianNB()\n",
    "model_count_gnb = gnb.fit(train_X, train_recommended)\n",
    "y_pred = model_count_gnb.predict(test_X)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), test_X.shape[0]))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n",
    "\n",
    "gnb_count = metrics.accuracy_score(test_recommended, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7359375\n",
      "0.696875\n",
      "0.7625\n",
      "0.7875\n",
      "0.696875\n"
     ]
    }
   ],
   "source": [
    "#Faz validação cruzada  - k = 10\n",
    "scores_count_cv_gnb = cross_val_score(model_count_gnb, train_X, y=train_recommended, cv=10)\n",
    "scores_count_cv_svc = cross_val_score(model_count_svc, train_X, y=train_recommended, cv=10)\n",
    "scores_count_cv_lr = cross_val_score(model_count_lr, train_X, y=train_recommended, cv=10)\n",
    "scores_count_cv_nb = cross_val_score(model_count_nb, train_X, y=train_recommended, cv=10)\n",
    "\n",
    "#faz média da validação cruzada para cada modelo\n",
    "count_cv_gnb = scores_count_cv_gnb.mean()\n",
    "count_cv_svc = scores_count_cv_svc.mean()\n",
    "count_cv_lr = scores_count_cv_lr.mean()\n",
    "count_cv_nb = scores_count_cv_nb.mean()\n",
    "\n",
    "#faz média global das validações cruzadas\n",
    "count_media = (count_cv_gnb + count_cv_svc + count_cv_lr + count_cv_nb) / 4 \n",
    "\n",
    "print(count_media)\n",
    "print(count_cv_gnb)\n",
    "print(count_cv_svc)\n",
    "print(count_cv_lr)\n",
    "print(count_cv_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.1 - Aprendizagem Automática com Pre-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cenário 1 \n",
    "Remoção da pontuação; Lowerization; Tokenization; Lemmatization; Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove pontuação\n",
    "import string\n",
    "\n",
    "train_text_pp = train_text\n",
    "for text in range(len(train_text)):\n",
    "    train_text_new = [\"\".join([char for char in text if char.isalnum() or char == \" \"]) for text in train_text_pp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cenario1 =[]\n",
    "\n",
    "for i, text in enumerate(train_text_new):\n",
    "    cenario1.append(text)\n",
    "    cenario1[i] = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz Tokenização\n",
    "for i in range(len(cenario1)):\n",
    "    cenario1[i] = cenario1[i].split()\n",
    "#print(cenario1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Faz Lemmatização\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(cenario1)):\n",
    "    for j in range(len(cenario1[i])):\n",
    "        cenario1[i][j] = lemmatizer.lemmatize(cenario1[i][j], 'v') #verbo\n",
    "        cenario1[i][j] = lemmatizer.lemmatize(cenario1[i][j], 'a') #adjetivo\n",
    "        cenario1[i][j] = lemmatizer.lemmatize(cenario1[i][j], 'n') #nomes\n",
    "        cenario1[i][j] = lemmatizer.lemmatize(cenario1[i][j], 's') #adjetivo saturado\n",
    "        cenario1[i][j] = lemmatizer.lemmatize(cenario1[i][j], 'r') #adverbio\n",
    "\n",
    "\n",
    "#print(cenario1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = stem.porter.PorterStemmer()\n",
    "\n",
    "for i in range(len(cenario1)):\n",
    "    for j in range(len(cenario1[i])):\n",
    "        cenario1[i][j] = stemmer.stem(cenario1[i][j])\n",
    "#print(cenario1[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untokenize(cenario1):\n",
    "    for tokens in cenario1:\n",
    "        yield ' '.join(tokens)\n",
    "\n",
    "untokenized_data_cenario1 = list(untokenize(cenario1))\n",
    "#print(untokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 1500) (80, 1500)\n"
     ]
    }
   ],
   "source": [
    "#converte palavras em números usando a abordagem do bag of words e define que vamos utilizar as 1500 palavras mais ocorrentes como features\n",
    "#Min_df = 3 significa que essas palavras têm de ocorrer em pelo menos 3 comentários\n",
    "#max_df = 0.7 significa que vamos incluir as palavras que ocorrem em no máximo 70% de todos os documentos, uma vez Palavras que ocorrem em quase todos os documentos geralmente não são adequadas para classificação porque não fornecem informações exclusivas sobre o documento.\n",
    "train_pp = untokenized_data_cenario1\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=3, max_df=0.7, stop_words= 'english')\n",
    "train_X_cenario1 = vectorizer.fit_transform(train_pp)\n",
    "test_X_cenario1 = vectorizer.transform(test_text)\n",
    "print(train_X_cenario1.shape, test_X_cenario1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_cenario1 = train_X_cenario1.todense()\n",
    "test_X_cenario1 = test_X_cenario1.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 31 out of 80\n",
      "Accuracy:  0.6125\n",
      "Precision:  0.6098484848484849\n",
      "Recall:  0.609365179132621\n",
      "F1-measure:  0.6095103133364824\n"
     ]
    }
   ],
   "source": [
    "#cria modelo \n",
    "nb = MultinomialNB()\n",
    "model_count_nb_cenario1 = nb.fit(train_X_cenario1,train_recommended)\n",
    "y_pred = model_count_nb_cenario1.predict(test_X_cenario1)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario1)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 33 out of 80\n",
      "Accuracy:  0.5875\n",
      "Precision:  0.6416666666666666\n",
      "Recall:  0.6068510370835952\n",
      "F1-measure:  0.567992145311733\n"
     ]
    }
   ],
   "source": [
    "#cria modelo \n",
    "lr = LogisticRegression(max_iter=500)\n",
    "model_count_lr_cenario1=lr.fit(train_X_cenario1, train_recommended)\n",
    "y_pred = model_count_lr_cenario1.predict(test_X_cenario1)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario1)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 35 out of 80\n",
      "Accuracy:  0.5625\n",
      "Precision:  0.6083333333333334\n",
      "Recall:  0.5817096165933375\n",
      "F1-measure:  0.5418098510882017\n"
     ]
    }
   ],
   "source": [
    "#cria modelo \n",
    "svmc = LinearSVC(max_iter=500)\n",
    "model_count_svc_cenario1 = svmc.fit(train_X_cenario1, train_recommended)\n",
    "y_pred = model_count_svc_cenario1.predict(test_X_cenario1)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario1)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 27 out of 80\n",
      "Accuracy:  0.6625\n",
      "Precision:  0.7249216300940439\n",
      "Recall:  0.680389692017599\n",
      "F1-measure:  0.6502024291497976\n"
     ]
    }
   ],
   "source": [
    "#cria modelo \n",
    "gnb = GaussianNB()\n",
    "model_count_gnb_cenario1 = gnb.fit(train_X_cenario1, train_recommended)\n",
    "y_pred = model_count_gnb_cenario1.predict(test_X_cenario1)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), test_X_cenario1.shape[0]))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72734375\n",
      "0.675\n",
      "0.7625\n",
      "0.790625\n",
      "0.68125\n"
     ]
    }
   ],
   "source": [
    "#Faz validação cruzada  - k = 10\n",
    "scores_count_cv_gnb_cenario1 = cross_val_score(model_count_gnb_cenario1, train_X_cenario1, y=train_recommended, cv=10)\n",
    "scores_count_cv_svc_cenario1 = cross_val_score(model_count_svc_cenario1, train_X_cenario1, y=train_recommended, cv=10)\n",
    "scores_count_cv_lr_cenario1 = cross_val_score(model_count_lr_cenario1, train_X_cenario1, y=train_recommended, cv=10)\n",
    "scores_count_cv_nb_cenario1 = cross_val_score(model_count_nb_cenario1, train_X_cenario1, y=train_recommended, cv=10)\n",
    "\n",
    "#faz média da validação cruzada para cada modelo\n",
    "count_cv_gnb_cenario1 = scores_count_cv_gnb_cenario1.mean()\n",
    "count_cv_svc_cenario1 = scores_count_cv_svc_cenario1.mean()\n",
    "count_cv_lr_cenario1 = scores_count_cv_lr_cenario1.mean()\n",
    "count_cv_nb_cenario1 = scores_count_cv_nb_cenario1.mean()\n",
    "\n",
    "#faz média global das validações cruzadas\n",
    "count_media_cenario1 = (count_cv_gnb_cenario1 + count_cv_svc_cenario1 + count_cv_lr_cenario1 + count_cv_nb_cenario1) / 4 \n",
    "\n",
    "print(count_media_cenario1)\n",
    "print(count_cv_gnb_cenario1)\n",
    "print(count_cv_svc_cenario1)\n",
    "print(count_cv_lr_cenario1)\n",
    "print(count_cv_nb_cenario1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cenário 2 \n",
    "Remoção da pontuação; Lowerization; Tokenization; Lemmatization;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove pontuação\n",
    "import string\n",
    "\n",
    "train_text_pp = train_text\n",
    "for text in range(len(train_text)):\n",
    "    train_text_new = [\"\".join([char for char in text if char.isalnum() or char == \" \"]) for text in train_text_pp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "cenario2 =[]\n",
    "\n",
    "for i, text in enumerate(train_text_new):\n",
    "    cenario2.append(text)\n",
    "    cenario2[i] = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz Tokenização\n",
    "for i in range(len(cenario2)):\n",
    "    cenario2[i] = cenario2[i].split()\n",
    "#print(cenario2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Faz Lemmatização\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(cenario2)):\n",
    "    for j in range(len(cenario2[i])):\n",
    "        cenario2[i][j] = lemmatizer.lemmatize(cenario2[i][j], 'v') #verbo\n",
    "        cenario2[i][j] = lemmatizer.lemmatize(cenario2[i][j], 'a') #adjetivo\n",
    "        cenario2[i][j] = lemmatizer.lemmatize(cenario2[i][j], 'n') #nomes\n",
    "        cenario2[i][j] = lemmatizer.lemmatize(cenario2[i][j], 's') #adjetivo saturado\n",
    "        cenario2[i][j] = lemmatizer.lemmatize(cenario2[i][j], 'r') #adverbio\n",
    "\n",
    "\n",
    "#print(cenario2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untokenize(cenario2):\n",
    "    for tokens in cenario2:\n",
    "        yield ' '.join(tokens)\n",
    "\n",
    "untokenized_data_cenario2 = list(untokenize(cenario2))\n",
    "#print(untokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 1500) (80, 1500)\n"
     ]
    }
   ],
   "source": [
    "#converte palavras em números usando a abordagem do bag of words e define que vamos utilizar as 1500 palavras mais ocorrentes como features\n",
    "#Min_df = 3 significa que essas palavras têm de ocorrer em pelo menos 3 comentários\n",
    "#max_df = 0.7 significa que vamos incluir as palavras que ocorrem em no máximo 70% de todos os documentos, uma vez Palavras que ocorrem em quase todos os documentos geralmente não são adequadas para classificação porque não fornecem informações exclusivas sobre o documento.\n",
    "train_pp = untokenized_data_cenario2\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=3, max_df=0.7, stop_words= 'english')\n",
    "train_X_cenario2 = vectorizer.fit_transform(train_pp)\n",
    "test_X_cenario2 = vectorizer.transform(test_text)\n",
    "print(train_X_cenario2.shape, test_X_cenario2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_cenario2 = train_X_cenario2.todense()\n",
    "test_X_cenario2 = test_X_cenario2.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 32 out of 80\n",
      "Accuracy:  0.6\n",
      "Precision:  0.5968253968253968\n",
      "Recall:  0.5958516656191075\n",
      "F1-measure:  0.595959595959596\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "nb = MultinomialNB()\n",
    "model_count_nb_cenario2 = nb.fit(train_X_cenario2,train_recommended)\n",
    "y_pred = model_count_nb_cenario2.predict(test_X_cenario2)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario2)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n",
    "\n",
    "nb_count_cenario2 = metrics.accuracy_score(test_recommended, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 25 out of 80\n",
      "Accuracy:  0.6875\n",
      "Precision:  0.71\n",
      "Recall:  0.6979886863607794\n",
      "F1-measure:  0.6850889623681311\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=500)\n",
    "model_count_lr_cenario2 = lr.fit(train_X_cenario2, train_recommended)\n",
    "y_pred = model_count_lr_cenario2.predict(test_X_cenario2)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario2)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 29 out of 80\n",
      "Accuracy:  0.6375\n",
      "Precision:  0.6566666666666666\n",
      "Recall:  0.647705845380264\n",
      "F1-measure:  0.6347031963470319\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "svmc = LinearSVC(max_iter=500)\n",
    "model_count_svc_cenario2 = svmc.fit(train_X_cenario2, train_recommended)\n",
    "y_pred = model_count_svc_cenario2.predict(test_X_cenario2)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario2)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 22 out of 80\n",
      "Accuracy:  0.725\n",
      "Precision:  0.7651991614255764\n",
      "Recall:  0.73852922690132\n",
      "F1-measure:  0.7206349206349206\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "gnb = GaussianNB()\n",
    "model_count_gnb_cenario2 = gnb.fit(train_X_cenario2, train_recommended)\n",
    "y_pred = model_count_gnb_cenario2.predict(test_X_cenario2)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario2)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n",
    "\n",
    "gnb_count_cenario2 = metrics.accuracy_score(test_recommended, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7281249999999999\n",
      "0.7\n",
      "0.76875\n",
      "0.76875\n",
      "0.675\n"
     ]
    }
   ],
   "source": [
    "#Faz validação cruzada  - k = 10\n",
    "scores_count_cv_gnb_cenario2 = cross_val_score(model_count_gnb_cenario2, train_X_cenario2, y=train_recommended, cv=10)\n",
    "scores_count_cv_svc_cenario2 = cross_val_score(model_count_svc_cenario2, train_X_cenario2, y=train_recommended, cv=10)\n",
    "scores_count_cv_lr_cenario2 = cross_val_score(model_count_lr_cenario2, train_X_cenario2, y=train_recommended, cv=10)\n",
    "scores_count_cv_nb_cenario2 = cross_val_score(model_count_nb_cenario2, train_X_cenario2, y=train_recommended, cv=10)\n",
    "\n",
    "#faz média da validação cruzada para cada modelo\n",
    "count_cv_gnb_cenario2 = scores_count_cv_gnb_cenario2.mean()\n",
    "count_cv_svc_cenario2 = scores_count_cv_svc_cenario2.mean()\n",
    "count_cv_lr_cenario2 = scores_count_cv_lr_cenario2.mean()\n",
    "count_cv_nb_cenario2 = scores_count_cv_nb_cenario2.mean()\n",
    "\n",
    "#faz média global das validações cruzadas\n",
    "count_media_cenario2 = (count_cv_gnb_cenario2 + count_cv_svc_cenario2 + count_cv_lr_cenario2 + count_cv_nb_cenario2) / 4 \n",
    "\n",
    "print(count_media_cenario2)\n",
    "print(count_cv_gnb_cenario2)\n",
    "print(count_cv_svc_cenario2)\n",
    "print(count_cv_lr_cenario2)\n",
    "print(count_cv_nb_cenario2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cenário 3\n",
    "\n",
    "Remoção da pontuação; Lowerization; Tokenization; Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove pontuação\n",
    "import string\n",
    "\n",
    "train_text_pp = train_text\n",
    "for text in range(len(train_text)):\n",
    "    train_text_new = [\"\".join([char for char in text if char.isalnum() or char == \" \"]) for text in train_text_pp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "cenario3 =[]\n",
    "\n",
    "for i, text in enumerate(train_text_new):\n",
    "    cenario3.append(text)\n",
    "    cenario3[i] = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz Tokenização\n",
    "for i in range(len(cenario3)):\n",
    "    cenario3[i] = cenario3[i].split()\n",
    "#print(cenario3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = stem.porter.PorterStemmer()\n",
    "\n",
    "for i in range(len(cenario3)):\n",
    "    for j in range(len(cenario3[i])):\n",
    "        cenario3[i][j] = stemmer.stem(cenario3[i][j])\n",
    "#print(cenario3[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untokenize(cenario3):\n",
    "    for tokens in cenario3:\n",
    "        yield ' '.join(tokens)\n",
    "\n",
    "untokenized_data_cenario3 = list(untokenize(cenario3))\n",
    "#print(untokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 1500) (80, 1500)\n"
     ]
    }
   ],
   "source": [
    "#converte palavras em números usando a abordagem do bag of words e define que vamos utilizar as 1500 palavras mais ocorrentes como features\n",
    "#Min_df = 3 significa que essas palavras têm de ocorrer em pelo menos 3 comentários\n",
    "#max_df = 0.7 significa que vamos incluir as palavras que ocorrem em no máximo 70% de todos os documentos, uma vez Palavras que ocorrem em quase todos os documentos geralmente não são adequadas para classificação porque não fornecem informações exclusivas sobre o documento.\n",
    "train_pp = untokenized_data_cenario3\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=3, max_df=0.7, stop_words= 'english')\n",
    "train_X_cenario3 = vectorizer.fit_transform(train_pp)\n",
    "test_X_cenario3 = vectorizer.transform(test_text)\n",
    "print(train_X_cenario3.shape, test_X_cenario3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_cenario3 = train_X_cenario3.todense()\n",
    "test_X_cenario3 = test_X_cenario3.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 27 out of 80\n",
      "Accuracy:  0.6625\n",
      "Precision:  0.6625\n",
      "Recall:  0.6634192331866751\n",
      "F1-measure:  0.6620247222656861\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "nb = MultinomialNB()\n",
    "model_count_nb_cenario3 = nb.fit(train_X_cenario3,train_recommended)\n",
    "y_pred = model_count_nb_cenario3.predict(test_X_cenario3)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario3)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n",
    "\n",
    "nb_count_cenario3 = metrics.accuracy_score(test_recommended, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 33 out of 80\n",
      "Accuracy:  0.5875\n",
      "Precision:  0.6550179211469533\n",
      "Recall:  0.6087366436203645\n",
      "F1-measure:  0.5628415300546448\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "lr = LogisticRegression(max_iter=500)\n",
    "model_count_lr_cenario3 = lr.fit(train_X_cenario3, train_recommended)\n",
    "y_pred = model_count_lr_cenario3.predict(test_X_cenario3)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario3)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n",
    "\n",
    "lr_count_cenario3 = metrics.accuracy_score(test_recommended, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 35 out of 80\n",
      "Accuracy:  0.5625\n",
      "Precision:  0.6083333333333334\n",
      "Recall:  0.5817096165933375\n",
      "F1-measure:  0.5418098510882017\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "svmc = LinearSVC(max_iter=500)\n",
    "model_count_svc_cenario3 = svmc.fit(train_X_cenario3, train_recommended)\n",
    "y_pred = model_count_svc_cenario3.predict(test_X_cenario3)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario3)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n",
    "\n",
    "svmc_count_cenario3 = metrics.accuracy_score(test_recommended, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 29 out of 320\n",
      "Accuracy:  0.6375\n",
      "Precision:  0.7083333333333333\n",
      "Recall:  0.6571338780641106\n",
      "F1-measure:  0.6203567337587956\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "gnb = GaussianNB()\n",
    "model_count_gnb_cenario3 = gnb.fit(train_X_cenario3, train_recommended)\n",
    "y_pred = model_count_gnb_cenario3.predict(test_X_cenario3)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), train_X_cenario3.shape[0]))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n",
    "\n",
    "gnb_count_cenario3 = metrics.accuracy_score(test_recommended, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7257812499999999\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "\n",
    "#Faz validação cruzada  - k = 10\n",
    "scores_count_cv_gnb_cenario3 = cross_val_score(model_count_gnb_cenario3, train_X_cenario3, y=train_recommended, cv=10)\n",
    "scores_count_cv_svc_cenario3 = cross_val_score(model_count_svc_cenario3, train_X_cenario3, y=train_recommended, cv=10)\n",
    "scores_count_cv_lr_cenario3 = cross_val_score(model_count_lr_cenario3, train_X_cenario3, y=train_recommended, cv=10)\n",
    "scores_count_cv_nb_cenario3 = cross_val_score(model_count_nb_cenario3, train_X_cenario3, y=train_recommended, cv=10)\n",
    "\n",
    "#faz média da validação cruzada para cada modelo\n",
    "count_cv_gnb_cenario3 = scores_count_cv_gnb_cenario3.mean()\n",
    "count_cv_svc_cenario3 = scores_count_cv_svc_cenario3.mean()\n",
    "count_cv_lr_cenario3 = scores_count_cv_lr_cenario3.mean()\n",
    "count_cv_nb_cenario3 = scores_count_cv_nb_cenario3.mean()\n",
    "\n",
    "#faz média global das validações cruzadas\n",
    "count_media_cenario3 = (count_cv_gnb_cenario3 + count_cv_svc_cenario3 + count_cv_lr_cenario3 + count_cv_nb_cenario3) / 4 \n",
    "\n",
    "print(count_media_cenario3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cenário 4\n",
    "\n",
    "Tokenization; Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz Tokenização\n",
    "cenario4 = train_text\n",
    "for i in range(len(cenario4)):\n",
    "    cenario4[i] = cenario4[i].split()\n",
    "#print(cenario4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = stem.porter.PorterStemmer()\n",
    "\n",
    "for i in range(len(cenario4)):\n",
    "    for j in range(len(cenario4[i])):\n",
    "        cenario4[i][j] = stemmer.stem(cenario4[i][j])\n",
    "#print(cenario4[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untokenize(cenario4):\n",
    "    for tokens in cenario4:\n",
    "        yield ' '.join(tokens)\n",
    "\n",
    "untokenized_data_cenario4 = list(untokenize(cenario4))\n",
    "#print(untokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 1500) (80, 1500)\n"
     ]
    }
   ],
   "source": [
    "#converte palavras em números usando a abordagem do bag of words e define que vamos utilizar as 1500 palavras mais ocorrentes como features\n",
    "#Min_df = 3 significa que essas palavras têm de ocorrer em pelo menos 3 comentários\n",
    "#max_df = 0.7 significa que vamos incluir as palavras que ocorrem em no máximo 70% de todos os documentos, uma vez Palavras que ocorrem em quase todos os documentos geralmente não são adequadas para classificação porque não fornecem informações exclusivas sobre o documento.\n",
    "train_pp = untokenized_data_cenario4\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=3, max_df=0.7, stop_words= 'english')\n",
    "train_X_cenario4 = vectorizer.fit_transform(train_pp)\n",
    "test_X_cenario4 = vectorizer.transform(test_text)\n",
    "print(train_X_cenario4.shape, test_X_cenario4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_cenario4 = train_X_cenario4.todense()\n",
    "test_X_cenario4 = test_X_cenario4.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 30 out of 80\n",
      "Accuracy:  0.625\n",
      "Precision:  0.6260162601626016\n",
      "Recall:  0.6266499057196732\n",
      "F1-measure:  0.6247654784240149\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "nb = MultinomialNB()\n",
    "model_count_nb_cenario4 = nb.fit(train_X_cenario4,train_recommended)\n",
    "y_pred = model_count_nb_cenario4.predict(test_X_cenario4)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario4)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 31 out of 80\n",
      "Accuracy:  0.6125\n",
      "Precision:  0.6908602150537635\n",
      "Recall:  0.6338780641106223\n",
      "F1-measure:  0.5893359827786058\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "\n",
    "lr = LogisticRegression(max_iter=500)\n",
    "model_count_lr_cenario4 = lr.fit(train_X_cenario4, train_recommended)\n",
    "y_pred = model_count_lr_cenario4.predict(test_X_cenario4)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario4)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 33 out of 80\n",
      "Accuracy:  0.5875\n",
      "Precision:  0.6308777429467085\n",
      "Recall:  0.6049654305468259\n",
      "F1-measure:  0.5724696356275304\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "svmc = LinearSVC(max_iter=500)\n",
    "model_count_svc_cenario4 = svmc.fit(train_X_cenario4, train_recommended)\n",
    "y_pred = model_count_svc_cenario4.predict(test_X_cenario4)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario4)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 24 out of 320\n",
      "Accuracy:  0.7\n",
      "Precision:  0.737246680642907\n",
      "Recall:  0.7133878064110621\n",
      "F1-measure:  0.6952380952380952\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "\n",
    "gnb = GaussianNB()\n",
    "model_count_gnb_cenario4 = gnb.fit(train_X_cenario4, train_recommended)\n",
    "y_pred = model_count_gnb_cenario4.predict(test_X_cenario4)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), train_X_cenario4.shape[0]))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7015625\n"
     ]
    }
   ],
   "source": [
    "#Faz validação cruzada  - k = 10\n",
    "scores_count_cv_gnb_cenario4 = cross_val_score(model_count_gnb_cenario4, train_X_cenario4, y=train_recommended, cv=10)\n",
    "scores_count_cv_svc_cenario4 = cross_val_score(model_count_svc_cenario4, train_X_cenario4, y=train_recommended, cv=10)\n",
    "scores_count_cv_lr_cenario4 = cross_val_score(model_count_lr_cenario4, train_X_cenario4, y=train_recommended, cv=10)\n",
    "scores_count_cv_nb_cenario4 = cross_val_score(model_count_nb_cenario4, train_X_cenario4, y=train_recommended, cv=10)\n",
    "\n",
    "#faz média da validação cruzada para cada modelo\n",
    "count_cv_gnb_cenario4 = scores_count_cv_gnb_cenario4.mean()\n",
    "count_cv_svc_cenario4 = scores_count_cv_svc_cenario4.mean()\n",
    "count_cv_lr_cenario4 = scores_count_cv_lr_cenario4.mean()\n",
    "count_cv_nb_cenario4 = scores_count_cv_nb_cenario4.mean()\n",
    "\n",
    "#faz média global das validações cruzadas\n",
    "count_media_cenario4 = (count_cv_gnb_cenario4 + count_cv_svc_cenario4 + count_cv_lr_cenario4 + count_cv_nb_cenario4) / 4 \n",
    "\n",
    "print(count_media_cenario4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7359375\n",
      "0.72734375\n",
      "0.7281249999999999\n",
      "0.7257812499999999\n",
      "0.7015625\n"
     ]
    }
   ],
   "source": [
    "print(count_media)\n",
    "print(count_media_cenario1)\n",
    "print(count_media_cenario2)\n",
    "print(count_media_cenario3)\n",
    "print(count_media_cenario4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Aprendizagem automática a usar a biblioteca SKLEARN - TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resetar o input \n",
    "df_train = pd.read_csv(\"TM/data/en-sentiment/SFU_Review_Corpus_train.csv\", encoding=\"utf-8\")\n",
    "df_test = pd.read_csv(\"TM/data/en-sentiment/SFU_Review_Corpus_test.csv\", encoding=\"utf-8\")\n",
    "\n",
    "test_text = df_test['text'].tolist()\n",
    "test_recommended = df_test['recommended'].tolist()\n",
    "train_text = df_train['text'].tolist()\n",
    "train_recommended = df_train['recommended'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 1500) (80, 1500)\n"
     ]
    }
   ],
   "source": [
    "#converte palavras em números usando a abordagem do bag of words e define que vamos utilizar as 1500 palavras mais ocorrentes como features\n",
    "#Min_df = 3 significa que essas palavras têm de ocorrer em pelo menos 3 comentários\n",
    "#max_df = 0.7 significa que vamos incluir as palavras que ocorrem em no máximo 70% de todos os documentos, uma vez Palavras que ocorrem em quase todos os documentos geralmente não são adequadas para classificação porque não fornecem informações exclusivas sobre o documento.\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1500, min_df=3, max_df=0.7, stop_words= 'english')\n",
    "train_X = vectorizer.fit_transform(train_text)\n",
    "test_X = vectorizer.transform(test_text)\n",
    "print(train_X.shape, test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.todense()\n",
    "test_X = test_X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 24 out of 80\n",
      "Accuracy:  0.7\n",
      "Precision:  0.7195523370638578\n",
      "Recall:  0.7096165933375236\n",
      "F1-measure:  0.6983029541169077\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "nb = MultinomialNB()\n",
    "model_tfidf_nb = nb.fit(train_X,train_recommended)\n",
    "y_pred = model_tfidf_nb.predict(test_X)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 24 out of 80\n",
      "Accuracy:  0.7\n",
      "Precision:  0.7275185936443542\n",
      "Recall:  0.711502199874293\n",
      "F1-measure:  0.696969696969697\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "lr = LogisticRegression(max_iter=500)\n",
    "model_tfidf_lr=lr.fit(train_X, train_recommended)\n",
    "y_pred = model_tfidf_lr.predict(test_X)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 18 out of 80\n",
      "Accuracy:  0.775\n",
      "Precision:  0.7841269841269841\n",
      "Recall:  0.781269641734758\n",
      "F1-measure:  0.7748592870544091\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "svmc = LinearSVC(max_iter=500)\n",
    "model_tfidf_svc = svmc.fit(train_X, train_recommended)\n",
    "y_pred = model_tfidf_svc.predict(test_X)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 20 out of 80\n",
      "Accuracy:  0.75\n",
      "Precision:  0.7510944340212633\n",
      "Recall:  0.7523570081709616\n",
      "F1-measure:  0.7498436522826766\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "gnb = GaussianNB()\n",
    "model_tfidf_gnb = gnb.fit(train_X, train_recommended)\n",
    "y_pred = model_tfidf_gnb.predict(test_X)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), test_X.shape[0]))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.734375\n"
     ]
    }
   ],
   "source": [
    "#Faz validação cruzada  - k = 10\n",
    "scores_tfidf_cv_gnb = cross_val_score(model_tfidf_gnb, train_X, y=train_recommended, cv=10)\n",
    "scores_tfidf_cv_svc = cross_val_score(model_tfidf_svc, train_X, y=train_recommended, cv=10)\n",
    "scores_tfidf_cv_lr = cross_val_score(model_tfidf_lr, train_X, y=train_recommended, cv=10)\n",
    "scores_tfidf_cv_nb = cross_val_score(model_tfidf_nb, train_X, y=train_recommended, cv=10)\n",
    "\n",
    "#faz média da validação cruzada para cada modelo\n",
    "tfidf_cv_gnb = scores_tfidf_cv_gnb.mean()\n",
    "tfidf_cv_svc = scores_tfidf_cv_svc.mean()\n",
    "tfidf_cv_lr = scores_tfidf_cv_lr.mean()\n",
    "tfidf_cv_nb = scores_tfidf_cv_nb.mean()\n",
    "\n",
    "#faz média global das validações cruzadas\n",
    "tfidf_media = (tfidf_cv_gnb + tfidf_cv_svc + tfidf_cv_lr + tfidf_cv_nb) / 4 \n",
    "\n",
    "print(tfidf_media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3.1 - Aprendizagem Automática com Pre-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cenário 1 \n",
    "Remoção da pontuação; Lowerization; Tokenization; Lemmatization; Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove pontuação\n",
    "import string\n",
    "\n",
    "train_text_pp = train_text\n",
    "for text in range(len(train_text)):\n",
    "    train_text_new = [\"\".join([char for char in text if char.isalnum() or char == \" \"]) for text in train_text_pp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "cenario1 =[]\n",
    "\n",
    "for i, text in enumerate(train_text_new):\n",
    "    cenario1.append(text)\n",
    "    cenario1[i] = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz Tokenização\n",
    "for i in range(len(cenario1)):\n",
    "    cenario1[i] = cenario1[i].split()\n",
    "#print(cenario1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Faz Lemmatização\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(cenario1)):\n",
    "    for j in range(len(cenario1[i])):\n",
    "        cenario1[i][j] = lemmatizer.lemmatize(cenario1[i][j], 'v') #verbo\n",
    "        cenario1[i][j] = lemmatizer.lemmatize(cenario1[i][j], 'a') #adjetivo\n",
    "        cenario1[i][j] = lemmatizer.lemmatize(cenario1[i][j], 'n') #nomes\n",
    "        cenario1[i][j] = lemmatizer.lemmatize(cenario1[i][j], 's') #adjetivo saturado\n",
    "        cenario1[i][j] = lemmatizer.lemmatize(cenario1[i][j], 'r') #adverbio\n",
    "\n",
    "\n",
    "#print(cenario1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = stem.porter.PorterStemmer()\n",
    "\n",
    "for i in range(len(cenario1)):\n",
    "    for j in range(len(cenario1[i])):\n",
    "        cenario1[i][j] = stemmer.stem(cenario1[i][j])\n",
    "#print(cenario1[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untokenize(cenario1):\n",
    "    for tokens in cenario1:\n",
    "        yield ' '.join(tokens)\n",
    "\n",
    "untokenized_data_cenario1 = list(untokenize(cenario1))\n",
    "#print(untokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 1500) (80, 1500)\n"
     ]
    }
   ],
   "source": [
    "#converte palavras em números usando a abordagem do bag of words e define que vamos utilizar as 1500 palavras mais ocorrentes como features\n",
    "#Min_df = 3 significa que essas palavras têm de ocorrer em pelo menos 3 comentários\n",
    "#max_df = 0.7 significa que vamos incluir as palavras que ocorrem em no máximo 70% de todos os documentos, uma vez Palavras que ocorrem em quase todos os documentos geralmente não são adequadas para classificação porque não fornecem informações exclusivas sobre o documento.\n",
    "train_pp = untokenized_data_cenario1\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1500, min_df=3, max_df=0.7, stop_words= 'english')\n",
    "train_X_cenario1 = vectorizer.fit_transform(train_pp)\n",
    "test_X_cenario1 = vectorizer.transform(test_text)\n",
    "print(train_X_cenario1.shape, test_X_cenario1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_cenario1 = train_X_cenario1.todense()\n",
    "test_X_cenario1 = test_X_cenario1.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 23 out of 80\n",
      "Accuracy:  0.7125\n",
      "Precision:  0.7184343434343434\n",
      "Recall:  0.7174732872407291\n",
      "F1-measure:  0.7124550711048601\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "nb = MultinomialNB()\n",
    "model_tfidf_nb_cenario1 = nb.fit(train_X_cenario1,train_recommended)\n",
    "y_pred = model_tfidf_nb_cenario1.predict(test_X_cenario1)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario1)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 32 out of 80\n",
      "Accuracy:  0.6\n",
      "Precision:  0.6414950419527079\n",
      "Recall:  0.61659333752357\n",
      "F1-measure:  0.5873629916183107\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "lr = LogisticRegression(max_iter=500)\n",
    "model_tfidf_lr_cenario1=lr.fit(train_X_cenario1, train_recommended)\n",
    "y_pred = model_tfidf_lr_cenario1.predict(test_X_cenario1)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario1)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 34 out of 80\n",
      "Accuracy:  0.575\n",
      "Precision:  0.6036363636363636\n",
      "Recall:  0.5895663104965431\n",
      "F1-measure:  0.5652173913043478\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "svmc = LinearSVC(max_iter=500)\n",
    "model_tfidf_svc_cenario1 = svmc.fit(train_X_cenario1, train_recommended)\n",
    "y_pred = model_tfidf_svc_cenario1.predict(test_X_cenario1)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario1)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 21 out of 80\n",
      "Accuracy:  0.7375\n",
      "Precision:  0.7362155388471178\n",
      "Recall:  0.7369578881206789\n",
      "F1-measure:  0.7364705882352942\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "gnb = GaussianNB()\n",
    "model_tfidf_gnb_cenario1 = gnb.fit(train_X_cenario1, train_recommended)\n",
    "y_pred = model_tfidf_gnb_cenario1.predict(test_X_cenario1)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), test_X_cenario1.shape[0]))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75625\n"
     ]
    }
   ],
   "source": [
    "#Faz validação cruzada  - k = 10\n",
    "scores_tfidf_cv_gnb_cenario1 = cross_val_score(model_tfidf_gnb_cenario1, train_X_cenario1, y=train_recommended, cv=10)\n",
    "scores_tfidf_cv_svc_cenario1 = cross_val_score(model_tfidf_svc_cenario1, train_X_cenario1, y=train_recommended, cv=10)\n",
    "scores_tfidf_cv_lr_cenario1 = cross_val_score(model_tfidf_lr_cenario1, train_X_cenario1, y=train_recommended, cv=10)\n",
    "scores_tfidf_cv_nb_cenario1 = cross_val_score(model_tfidf_nb_cenario1, train_X_cenario1, y=train_recommended, cv=10)\n",
    "\n",
    "#faz média da validação cruzada para cada modelo\n",
    "tfidf_cv_gnb_cenario1 = scores_tfidf_cv_gnb_cenario1.mean()\n",
    "tfidf_cv_svc_cenario1 = scores_tfidf_cv_svc_cenario1.mean()\n",
    "tfidf_cv_lr_cenario1 = scores_tfidf_cv_lr_cenario1.mean()\n",
    "tfidf_cv_nb_cenario1 = scores_tfidf_cv_nb_cenario1.mean()\n",
    "\n",
    "#faz média global das validações cruzadas\n",
    "tfidf_media_cenario1 = (tfidf_cv_gnb_cenario1 + tfidf_cv_svc_cenario1 + tfidf_cv_lr_cenario1 + tfidf_cv_nb_cenario1) / 4 \n",
    "\n",
    "print(tfidf_media_cenario1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cenário 2 \n",
    "Remoção da pontuação; Lowerization; Tokenization; Lemmatization;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove pontuação\n",
    "import string\n",
    "\n",
    "train_text_pp = train_text\n",
    "for text in range(len(train_text)):\n",
    "    train_text_new = [\"\".join([char for char in text if char.isalnum() or char == \" \"]) for text in train_text_pp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "cenario2 =[]\n",
    "\n",
    "for i, text in enumerate(train_text_new):\n",
    "    cenario2.append(text)\n",
    "    cenario2[i] = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz Tokenização\n",
    "for i in range(len(cenario2)):\n",
    "    cenario2[i] = cenario2[i].split()\n",
    "#print(cenario2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Faz Lemmatização\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(cenario2)):\n",
    "    for j in range(len(cenario2[i])):\n",
    "        cenario2[i][j] = lemmatizer.lemmatize(cenario2[i][j], 'v') #verbo\n",
    "        cenario2[i][j] = lemmatizer.lemmatize(cenario2[i][j], 'a') #adjetivo\n",
    "        cenario2[i][j] = lemmatizer.lemmatize(cenario2[i][j], 'n') #nomes\n",
    "        cenario2[i][j] = lemmatizer.lemmatize(cenario2[i][j], 's') #adjetivo saturado\n",
    "        cenario2[i][j] = lemmatizer.lemmatize(cenario2[i][j], 'r') #adverbio\n",
    "\n",
    "\n",
    "#print(cenario2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untokenize(cenario2):\n",
    "    for tokens in cenario2:\n",
    "        yield ' '.join(tokens)\n",
    "\n",
    "untokenized_data_cenario2 = list(untokenize(cenario2))\n",
    "#print(untokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 1500) (80, 1500)\n"
     ]
    }
   ],
   "source": [
    "#converte palavras em números usando a abordagem do bag of words e define que vamos utilizar as 1500 palavras mais ocorrentes como features\n",
    "#Min_df = 3 significa que essas palavras têm de ocorrer em pelo menos 3 comentários\n",
    "#max_df = 0.7 significa que vamos incluir as palavras que ocorrem em no máximo 70% de todos os documentos, uma vez Palavras que ocorrem em quase todos os documentos geralmente não são adequadas para classificação porque não fornecem informações exclusivas sobre o documento.\n",
    "train_pp = untokenized_data_cenario2\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1500, min_df=3, max_df=0.7, stop_words= 'english')\n",
    "train_X_cenario2 = vectorizer.fit_transform(train_pp)\n",
    "test_X_cenario2 = vectorizer.transform(test_text)\n",
    "print(train_X_cenario2.shape, test_X_cenario2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_cenario2 = train_X_cenario2.todense()\n",
    "test_X_cenario2 = test_X_cenario2.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 25 out of 80\n",
      "Accuracy:  0.6875\n",
      "Precision:  0.6975703324808185\n",
      "Recall:  0.6942174732872407\n",
      "F1-measure:  0.6870599280237835\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "nb = MultinomialNB()\n",
    "model_tfidf_nb_cenario2 = nb.fit(train_X_cenario2,train_recommended)\n",
    "y_pred = model_tfidf_nb_cenario2.predict(test_X_cenario2)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario2)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 23 out of 80\n",
      "Accuracy:  0.7125\n",
      "Precision:  0.7231457800511509\n",
      "Recall:  0.7193588937774984\n",
      "F1-measure:  0.7120951337818808\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "lr = LogisticRegression(max_iter=500)\n",
    "model_tfidf_lr_cenario2=lr.fit(train_X_cenario2, train_recommended)\n",
    "y_pred = model_tfidf_lr_cenario2.predict(test_X_cenario2)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario2)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 23 out of 80\n",
      "Accuracy:  0.7125\n",
      "Precision:  0.7231457800511509\n",
      "Recall:  0.7193588937774984\n",
      "F1-measure:  0.7120951337818808\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "svmc = LinearSVC(max_iter=500)\n",
    "model_tfidf_svc_cenario2 = svmc.fit(train_X_cenario2, train_recommended)\n",
    "y_pred = model_tfidf_svc_cenario2.predict(test_X_cenario2)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario2)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 19 out of 80\n",
      "Accuracy:  0.7625\n",
      "Precision:  0.7625\n",
      "Recall:  0.7639849151477058\n",
      "F1-measure:  0.7621655452980753\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "gnb = GaussianNB()\n",
    "model_tfidf_gnb_cenario2 = gnb.fit(train_X_cenario2, train_recommended)\n",
    "y_pred = model_tfidf_gnb_cenario2.predict(test_X_cenario2)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario2)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7445312499999999\n"
     ]
    }
   ],
   "source": [
    "#Faz validação cruzada  - k = 10\n",
    "scores_tfidf_cv_gnb_cenario2 = cross_val_score(model_tfidf_gnb_cenario2, train_X_cenario2, y=train_recommended, cv=10)\n",
    "scores_tfidf_cv_svc_cenario2 = cross_val_score(model_tfidf_svc_cenario2, train_X_cenario2, y=train_recommended, cv=10)\n",
    "scores_tfidf_cv_lr_cenario2 = cross_val_score(model_tfidf_lr_cenario2, train_X_cenario2, y=train_recommended, cv=10)\n",
    "scores_tfidf_cv_nb_cenario2 = cross_val_score(model_tfidf_nb_cenario2, train_X_cenario2, y=train_recommended, cv=10)\n",
    "\n",
    "#faz média da validação cruzada para cada modelo\n",
    "tfidf_cv_gnb_cenario2 = scores_tfidf_cv_gnb_cenario2.mean()\n",
    "tfidf_cv_svc_cenario2 = scores_tfidf_cv_svc_cenario2.mean()\n",
    "tfidf_cv_lr_cenario2 = scores_tfidf_cv_lr_cenario2.mean()\n",
    "tfidf_cv_nb_cenario2 = scores_tfidf_cv_nb_cenario2.mean()\n",
    "\n",
    "#faz média global das validações cruzadas\n",
    "tfidf_media_cenario2 = (tfidf_cv_gnb_cenario2 + tfidf_cv_svc_cenario2 + tfidf_cv_lr_cenario2 + tfidf_cv_nb_cenario2) / 4 \n",
    "\n",
    "print(tfidf_media_cenario2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cenário 3\n",
    "\n",
    "Remoção da pontuação; Lowerization; Tokenization; Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove pontuação\n",
    "import string\n",
    "\n",
    "train_text_pp = train_text\n",
    "for text in range(len(train_text)):\n",
    "    train_text_new = [\"\".join([char for char in text if char.isalnum() or char == \" \"]) for text in train_text_pp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "cenario3 =[]\n",
    "\n",
    "for i, text in enumerate(train_text_new):\n",
    "    cenario3.append(text)\n",
    "    cenario3[i] = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz Tokenização\n",
    "for i in range(len(cenario3)):\n",
    "    cenario3[i] = cenario3[i].split()\n",
    "#print(cenario3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = stem.porter.PorterStemmer()\n",
    "\n",
    "for i in range(len(cenario3)):\n",
    "    for j in range(len(cenario3[i])):\n",
    "        cenario3[i][j] = stemmer.stem(cenario3[i][j])\n",
    "#print(cenario3[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untokenize(cenario3):\n",
    "    for tokens in cenario3:\n",
    "        yield ' '.join(tokens)\n",
    "\n",
    "untokenized_data_cenario3 = list(untokenize(cenario3))\n",
    "#print(untokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 1500) (80, 1500)\n"
     ]
    }
   ],
   "source": [
    "#converte palavras em números usando a abordagem do bag of words e define que vamos utilizar as 1500 palavras mais ocorrentes como features\n",
    "#Min_df = 3 significa que essas palavras têm de ocorrer em pelo menos 3 comentários\n",
    "#max_df = 0.7 significa que vamos incluir as palavras que ocorrem em no máximo 70% de todos os documentos, uma vez Palavras que ocorrem em quase todos os documentos geralmente não são adequadas para classificação porque não fornecem informações exclusivas sobre o documento.\n",
    "train_pp = untokenized_data_cenario3\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1500, min_df=3, max_df=0.7, stop_words= 'english')\n",
    "train_X_cenario3 = vectorizer.fit_transform(train_pp)\n",
    "test_X_cenario3 = vectorizer.transform(test_text)\n",
    "print(train_X_cenario3.shape, test_X_cenario3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_cenario3 = train_X_cenario3.todense()\n",
    "test_X_cenario3 = test_X_cenario3.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 25 out of 80\n",
      "Accuracy:  0.6875\n",
      "Precision:  0.6931818181818181\n",
      "Recall:  0.6923318667504714\n",
      "F1-measure:  0.6874511642444132\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "\n",
    "nb = MultinomialNB()\n",
    "model_tfidf_nb_cenario3 = nb.fit(train_X_cenario3,train_recommended)\n",
    "y_pred = model_tfidf_nb_cenario3.predict(test_X_cenario3)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario3)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 32 out of 80\n",
      "Accuracy:  0.6\n",
      "Precision:  0.652138821630347\n",
      "Recall:  0.6184789440603394\n",
      "F1-measure:  0.5833333333333334\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "lr = LogisticRegression(max_iter=500)\n",
    "model_tfidf_lr_cenario3=lr.fit(train_X_cenario3, train_recommended)\n",
    "y_pred = model_tfidf_lr_cenario3.predict(test_X_cenario3)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario3)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 32 out of 80\n",
      "Accuracy:  0.6\n",
      "Precision:  0.6327272727272727\n",
      "Recall:  0.6147077309868008\n",
      "F1-measure:  0.5907928388746803\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "svmc = LinearSVC(max_iter=500)\n",
    "model_tfidf_svc_cenario3 = svmc.fit(train_X_cenario3, train_recommended)\n",
    "y_pred = model_tfidf_svc_cenario3.predict(test_X_cenario3)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario3)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 22 out of 320\n",
      "Accuracy:  0.725\n",
      "Precision:  0.7291011942174732\n",
      "Recall:  0.7291011942174732\n",
      "F1-measure:  0.725\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "gnb = GaussianNB()\n",
    "model_tfidf_gnb_cenario3 = gnb.fit(train_X_cenario3, train_recommended)\n",
    "y_pred = model_tfidf_gnb_cenario3.predict(test_X_cenario3)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), train_X_cenario3.shape[0]))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n",
    "\n",
    "gnb_tfidf_cenario3 = metrics.accuracy_score(test_recommended, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74765625\n"
     ]
    }
   ],
   "source": [
    "#Faz validação cruzada  - k = 10\n",
    "scores_tfidf_cv_gnb_cenario3 = cross_val_score(model_tfidf_gnb_cenario3, train_X_cenario3, y=train_recommended, cv=10)\n",
    "scores_tfidf_cv_svc_cenario3 = cross_val_score(model_tfidf_svc_cenario3, train_X_cenario3, y=train_recommended, cv=10)\n",
    "scores_tfidf_cv_lr_cenario3 = cross_val_score(model_tfidf_lr_cenario3, train_X_cenario3, y=train_recommended, cv=10)\n",
    "scores_tfidf_cv_nb_cenario3 = cross_val_score(model_tfidf_nb_cenario3, train_X_cenario3, y=train_recommended, cv=10)\n",
    "\n",
    "#faz média da validação cruzada para cada modelo\n",
    "tfidf_cv_gnb_cenario3 = scores_tfidf_cv_gnb_cenario3.mean()\n",
    "tfidf_cv_svc_cenario3 = scores_tfidf_cv_svc_cenario3.mean()\n",
    "tfidf_cv_lr_cenario3 = scores_tfidf_cv_lr_cenario3.mean()\n",
    "tfidf_cv_nb_cenario3 = scores_tfidf_cv_nb_cenario3.mean()\n",
    "\n",
    "#faz média global das validações cruzadas\n",
    "tfidf_media_cenario3 = (tfidf_cv_gnb_cenario3 + tfidf_cv_svc_cenario3 + tfidf_cv_lr_cenario3 + tfidf_cv_nb_cenario3) / 4 \n",
    "\n",
    "print(tfidf_media_cenario3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cenário 4\n",
    "\n",
    "Tokenization; Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz Tokenização\n",
    "cenario4 = train_text\n",
    "for i in range(len(cenario4)):\n",
    "    cenario4[i] = cenario4[i].split()\n",
    "#print(cenario4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = stem.porter.PorterStemmer()\n",
    "\n",
    "for i in range(len(cenario4)):\n",
    "    for j in range(len(cenario4[i])):\n",
    "        cenario4[i][j] = stemmer.stem(cenario4[i][j])\n",
    "#print(cenario4[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untokenize(cenario4):\n",
    "    for tokens in cenario4:\n",
    "        yield ' '.join(tokens)\n",
    "\n",
    "untokenized_data_cenario4 = list(untokenize(cenario4))\n",
    "#print(untokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 1500) (80, 1500)\n"
     ]
    }
   ],
   "source": [
    "#converte palavras em números usando a abordagem do bag of words e define que vamos utilizar as 1500 palavras mais ocorrentes como features\n",
    "#Min_df = 3 significa que essas palavras têm de ocorrer em pelo menos 3 comentários\n",
    "#max_df = 0.7 significa que vamos incluir as palavras que ocorrem em no máximo 70% de todos os documentos, uma vez Palavras que ocorrem em quase todos os documentos geralmente não são adequadas para classificação porque não fornecem informações exclusivas sobre o documento.\n",
    "train_pp = untokenized_data_cenario4\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1500, min_df=3, max_df=0.7, stop_words= 'english')\n",
    "train_X_cenario4 = vectorizer.fit_transform(train_pp)\n",
    "test_X_cenario4 = vectorizer.transform(test_text)\n",
    "print(train_X_cenario4.shape, test_X_cenario4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_cenario4 = train_X_cenario4.todense()\n",
    "test_X_cenario4 = test_X_cenario4.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 24 out of 80\n",
      "Accuracy:  0.7\n",
      "Precision:  0.737246680642907\n",
      "Recall:  0.7133878064110621\n",
      "F1-measure:  0.6952380952380952\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "nb = MultinomialNB()\n",
    "model_tfidf_nb_cenario4 = nb.fit(train_X_cenario4,train_recommended)\n",
    "y_pred = model_tfidf_nb_cenario4.predict(test_X_cenario4)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario4)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 26 out of 80\n",
      "Accuracy:  0.675\n",
      "Precision:  0.7489911218724778\n",
      "Recall:  0.6939032055311125\n",
      "F1-measure:  0.6614583333333333\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "lr = LogisticRegression(max_iter=500)\n",
    "model_tfidf_lr_cenario4=lr.fit(train_X_cenario4, train_recommended)\n",
    "y_pred = model_tfidf_lr_cenario4.predict(test_X_cenario4)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario4)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 26 out of 80\n",
      "Accuracy:  0.675\n",
      "Precision:  0.7330282227307399\n",
      "Recall:  0.6920175989943432\n",
      "F1-measure:  0.6647324306898774\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "svmc = LinearSVC(max_iter=500)\n",
    "model_tfidf_svc_cenario4 = svmc.fit(train_X_cenario4, train_recommended)\n",
    "y_pred = model_tfidf_svc_cenario4.predict(test_X_cenario4)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario4)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points: 21 out of 80\n",
      "Accuracy:  0.7375\n",
      "Precision:  0.7436868686868687\n",
      "Recall:  0.7426147077309868\n",
      "F1-measure:  0.7374589779653071\n"
     ]
    }
   ],
   "source": [
    "#Cria modelo\n",
    "gnb = GaussianNB()\n",
    "model_tfidf_gnb_cenario4 = gnb.fit(train_X_cenario4, train_recommended)\n",
    "y_pred = model_tfidf_gnb_cenario4.predict(test_X_cenario4)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((test_recommended!=y_pred).sum(), len(test_X_cenario4)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_recommended, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(test_recommended, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(test_recommended, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7343750000000001\n"
     ]
    }
   ],
   "source": [
    "#Faz validação cruzada  - k = 10\n",
    "scores_tfidf_cv_gnb_cenario4 = cross_val_score(model_tfidf_gnb_cenario4, train_X_cenario4, y=train_recommended, cv=10)\n",
    "scores_tfidf_cv_svc_cenario4 = cross_val_score(model_tfidf_svc_cenario4, train_X_cenario4, y=train_recommended, cv=10)\n",
    "scores_tfidf_cv_lr_cenario4 = cross_val_score(model_tfidf_lr_cenario4, train_X_cenario4, y=train_recommended, cv=10)\n",
    "scores_tfidf_cv_nb_cenario4 = cross_val_score(model_tfidf_nb_cenario4, train_X_cenario4, y=train_recommended, cv=10)\n",
    "\n",
    "#faz média da validação cruzada para cada modelo\n",
    "tfidf_cv_gnb_cenario4 = scores_tfidf_cv_gnb_cenario4.mean()\n",
    "tfidf_cv_svc_cenario4 = scores_tfidf_cv_svc_cenario4.mean()\n",
    "tfidf_cv_lr_cenario4 = scores_tfidf_cv_lr_cenario4.mean()\n",
    "tfidf_cv_nb_cenario4 = scores_tfidf_cv_nb_cenario4.mean()\n",
    "\n",
    "#faz média global das validações cruzadas\n",
    "tfidf_media_cenario4 = (tfidf_cv_gnb_cenario4 + tfidf_cv_svc_cenario4 + tfidf_cv_lr_cenario4 + tfidf_cv_nb_cenario4) / 4 \n",
    "\n",
    "print(tfidf_media_cenario4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.734375\n",
      "0.75625\n",
      "0.7445312499999999\n",
      "0.74765625\n",
      "0.7343750000000001\n"
     ]
    }
   ],
   "source": [
    "#print médias global de cada experiência com o TF-IDF\n",
    "print(tfidf_media)\n",
    "print(tfidf_media_cenario1)\n",
    "print(tfidf_media_cenario2)\n",
    "print(tfidf_media_cenario3)\n",
    "print(tfidf_media_cenario4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7359375\n",
      "0.72734375\n",
      "0.7281249999999999\n",
      "0.7257812499999999\n",
      "0.7015625\n"
     ]
    }
   ],
   "source": [
    "#print médias global de cada experiência com vectorização por contagem de palavras\n",
    "print(count_media)\n",
    "print(count_media_cenario1)\n",
    "print(count_media_cenario2)\n",
    "print(count_media_cenario3)\n",
    "print(count_media_cenario4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
